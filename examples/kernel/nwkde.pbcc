#ifndef NWKDE_PBCC
#define NWKDE_PBCC

#include "../simple/macros.h"
#include "../simple/reduce.pbcc"
#include "nwkde.h"
#include "nwkdeGenerators.pbcc"
#include "nwkdeMetric.pbcc"
#include "utils.pbcc"

%{

/* wrap relative difference into [-180,180]
 * do most of the arithmetic in integers for speed
 * P360 and M360 indicate "plus 360" and "minus 360" */
inline ElementT wrapWindDirDiff(ElementT diff)
{
    /* add 360 *before* cast to round towards -INF instead of towards 0 */
    int diffIntP360 = (int) (diff + 360);

    /* add 180 to ensure modulo result is positive */
    int diffIntWrapP360 = ((diffIntP360 + 180) % 360) + 180;

    return diff + (diffIntWrapP360 - diffIntP360);
}

%}

transform NWKDECheckInputs
from TRAINDATA[m,n], TRAINX[2,p], TRAINY[2], TRAININDICES[l],
     TESTDATA[m2,n2], TESTX[2,p], TESTINDICES[q]
to INPUTSCHECKED
{
    INPUTSCHECKED
    from (TRAININDICES trainIndices, TRAINX trainX, TRAINY trainY,
          TESTINDICES testIndices, TESTX testX)
    {
        ElementT min, max, min2, max2;

        findMinAndMax(&min, &max, trainX.col(0));
        fprintf(stderr, "trainX.col(0) range: (%g, %g)\n", min, max);
        JASSERT (min >= 0 && max < m)(m).Text("trainX.col(0) out of bounds");

        fprintf(stderr, "trainY.cell(0): %g\n", trainY.cell(0));
        JASSERT (trainY.cell(0) >= 0 && trainY.cell(0) < m)
                (m).Text("trainY.cell(0) out of bounds");

        findMinAndMax(&min, &max, testX.col(0));
        fprintf(stderr, "testX.col(0) range: (%g, %g)\n", min, max);
        JASSERT (min >= 0 && max < m2)(m2).Text("testX.col(0) out of bounds");

        findMinAndMax(&min, &max, trainIndices);
        findMinAndMax(&min2, &max2, trainX.col(1));
        fprintf(stderr, "trainIndices + trainX.col(1) range: (%g, %g)\n",
                min + min2, max + max2);
        JASSERT (min + min2 >= 0 && max + max2 < n)
                (n).Text("trainIndices + trainX.col(1) out of bounds");

        fprintf(stderr, "trainIndices + trainY.cell(1) range: (%g, %g)\n",
                min + trainY.cell(1), max + trainY.cell(1));
        JASSERT (min + trainY.cell(1) >= 0 && max + trainY.cell(1) < n)
                (n).Text("trainIndices + trainY.cell(1) out of bounds");

        findMinAndMax(&min, &max, testIndices);
        findMinAndMax(&min2, &max2, testX.col(1));
        fprintf(stderr, "testIndices + testX.col(1) range: (%g, %g)\n",
               min + min2, max + max2);
        JASSERT (min + min2 >= 0 && max + max2 < n2)
                (n).Text("testIndices + testX.col(1) out of bounds");
    }
}

/*  TRAINDATA - block of data: n time slices, m variables per time slice
    TRAININDICES - l time indices into TRAINDATA to use for training

    TESTDATA - block of data: n2 time slices, m2 variables per time slice
    TESTINDICES - q indices into TESTDATA to evaluate the regression

    For each time index in TRAININDICES or TESTINDICES, we associate
    a p-dim vector of predictors for use during regression.  Each predictor
    variable is represented by a 2-element row in TRAINX.  The first element
    of each row is the column index (into TRAINDATA) of the variable.  The
    second element is the time offset relative to the current time index.

    In this way, we can build predictor vectors that contain overlapping data
    for different time indices.  TRAINY specifies the output variable location
    in TRAINDATA for each time index.  Finally, TESTX specifies how to
    construct the predictor vector in TESTDATA.

    DIRFLAGS - indicates whether TRAINDATA column corresponds to a wind
               direction \in [0, 360]
    KERNELWIDTHS - width of the kernel function to use for a data column
    MASKWIDTH - width of training mask.  This tells the transform to omit
                training indices less than MASKWIDTH indices of the test index
                when computing the regression.  Note: to be used when TESTDATA
                is equal to TRAINDATA.  May set to 0 (no mask effect) if
                TESTDATA is separate from TRAINDATA.

    SQDIFFS - squared differences for each predictor for each train-test
              point pair
    WEIGHTS - weights computed with Gaussian kernel function for each
              train-test point pair
    PARTIALS - weighted output partial sums
*/

//#define INCLUDE_METHOD2
#define INCLUDE_METHOD3

transform NWKDEBase
from TRAINDATA[m,n], TRAINX[2,p], TRAINY[2], TRAININDICES[l],
     TESTDATA[m2,n2], TESTX[2,p], TESTINDICES[q],
     DIRFLAGS[m], KERNELWIDTHS[m], MASKWIDTH
to RESULT[q]
through INITFLAGS[l], SKIPINDEXFLAGS[l], SQDIFFS[p,l,q], WEIGHTS[l,q], PARTIALS[l,q]
{

#ifdef INCLUDE_METHOD1 // TODO: This method seems to confuse the scheduler

    /* METHOD 1: compute PARTIALS by exposing the most fine-grained
       parallelism.  this method may be less cache-efficient depending on
       execution ordering. */

    // initialize skip index flags to 0
    to   (INITFLAGS.cell(j) initFlag,
          SKIPINDEXFLAGS.cell(j) skipIndexFlag)
    from ()
    {
        skipIndexFlag = 0;
        initFlag = 1;
    }

    // Note: rule depends on initFlag so that skipIndexFlag defaults to 0
    to   (SQDIFFS.cell(i,j,k) sqDiff,
          SKIPINDEXFLAGS.cell(j) skipIndexFlag)
    from (INITFLAGS.cell(j) initFlag,
          TRAINDATA trainData, TRAINX.row(i) trainX,
          TRAININDICES.cell(j) trainIndex,
          TESTDATA testData, TESTX.row(i) testX,
          TESTINDICES.cell(k) testIndex,
          DIRFLAGS dirFlags, KERNELWIDTHS kernelWidths)
    {
        IndexT dirFlag;
        ElementT kernelWidth, trainPoint, testPoint, diff;

        dirFlag     =     dirFlags.cell(trainX.cell(0));
        kernelWidth = kernelWidths.cell(trainX.cell(0));

        trainPoint = lookup(trainData, trainX, trainIndex);
        testPoint  = lookup( testData,  testX,  testIndex);

        // skip this variable if nan is detected in the test point
        if (ISNAN(testPoint)) {
            sqDiff = 0;
            return;
        }

        // skip this training index if nan is detected in the train point
        if (ISNAN(trainPoint)) {
            skipIndexFlag = 1;
            return;
        }

        diff = trainPoint - testPoint;

        if (dirFlag) {
            diff = wrapWindDirDiff(diff);
        }

        // normalize according to kernel width
        diff /= kernelWidth;

        // return squared difference
        sqDiff = diff * diff;
#ifdef DEBUG
        fprintf(stderr, "method 1:  sqdiff(%d, %d, %d) = %g\n", i, j, k, sqDiff);
#endif
    }

    to (WEIGHTS.cell(j,k) weight)
    from (SQDIFFS.region(0, j,   k,
                         p ,j+1, k+1) sqDiffs,
          TRAININDICES.cell(j) trainIndex,
          TESTINDICES.cell(k) testIndex,
          MASKWIDTH maskWidth,
          SKIPINDEXFLAGS.cell(j) skipIndexFlag)
    {
        if (!skipIndexFlag &&
            (trainIndex <= testIndex - maskWidth ||
             trainIndex >= testIndex + maskWidth)) {
            ReduceAdd(weight, sqDiffs.slice(2,0).slice(1,0));
            weight = exp(-((ElementT) weight));
        } else {
            weight = 0;
        }
#ifdef DEBUG
        fprintf(stderr, "method 1:  weight(%d, %d) = %g\n", j, k, weight);
#endif
    }

    to (PARTIALS.cell(j,k) partial)
    from (TRAINDATA trainData,
          TRAININDICES.cell(j) trainIndex,
          TRAINY trainY,
          WEIGHTS.cell(j,k) weight)
    {
        outputVal = lookup(trainData, trainY, trainIndex);
        partial = ISNAN(outputVal) ? 0 : weight * outputVal;
#ifdef DEBUG
        fprintf(stderr, "method 1: partial(%d, %d) = %g\n", j, k, partial);
#endif
    }

    /* METHODS 1 & 2: Once we have the PARTIALS and WEIGHTS, we can compute
       RESULT */

    to (RESULT.cell(k) result)
    from (PARTIALS.row(k) partials,
          WEIGHTS.row(k) weights)
    {
        ElementT totalWeight;
        ReduceAdd1D(result, partials);
        ReduceAdd1D(totalWeight, weights);
        result /= totalWeight;
#ifdef DEBUG
        fprintf(stderr, "Output %d = %g\n", k, result);
#endif // DEBUG
    }

#endif // INCLUDE_METHOD1

#ifdef INCLUDE_METHOD2

    /* METHOD 2: Compute weights and partials directly with one pass through the data */

    to   (PARTIALS.cell(j,k) partial,
          WEIGHTS.cell(j,k) weight)
    from (TRAINDATA trainData, TRAINX trainX,
          TRAINY trainY, TRAININDICES.cell(j) trainIndex,
          TESTDATA testData, TESTX testX, TESTINDICES.cell(k) testIndex,
          DIRFLAGS dirFlags, KERNELWIDTHS kernelWidths, MASKWIDTH maskWidth)
    {
        IndexT i, dirFlag;
        ElementT outputVal, kernelWidth, trainPoint, testPoint, diff, sum;

        outputVal = lookup(trainData, trainY, trainIndex);

        // skip current training index if it is too close to the test
        // index or if the training output value is a NAN
        if ((trainIndex > testIndex - maskWidth &&
             trainIndex < testIndex + maskWidth) ||
            ISNAN(outputVal)) {
            weight = partial = 0;
            return;
        }

        // loop over predictor variables to calculate training weight
        sum = 0;
        for (i = 0; i < trainX.size(1); ++i) {
            dirFlag     =     dirFlags.cell(trainX.cell(0, i));
            kernelWidth = kernelWidths.cell(trainX.cell(0, i));
            trainPoint = lookup(trainData, trainX.row(i), trainIndex);
            testPoint  = lookup( testData,  testX.row(i),  testIndex);

            // skip this variable if nan is detected in the test point
            if (ISNAN(testPoint)) { continue; }

            // skip this training index if nan is detected
            if (ISNAN(trainPoint)) { weight = partial = 0; return; }

            // compute diff (and wrap if dirFlag is true)
            diff = trainPoint - testPoint;
            if (dirFlag) { diff = wrapWindDirDiff(diff); }

            // normalize and save squared difference
            diff /= kernelWidth;
            sum += diff * diff;
        }

        weight = exp(-((ElementT) sum));
        partial = weight * outputVal;
#ifdef DEBUG
        fprintf(stderr, "method 2:  weight(%d, %d) = %g\n", j, k, weight);
        fprintf(stderr, "method 2: partial(%d, %d) = %g\n", j, k, partial);
#endif
    }

    /* METHODS 1 & 2: Once we have the PARTIALS and WEIGHTS, we can compute
       RESULT */

    to (RESULT.cell(k) result)
    from (PARTIALS.row(k) partials,
          WEIGHTS.row(k) weights)
    {
        ElementT totalWeight;
        ReduceAdd1D(result, partials);
        ReduceAdd1D(totalWeight, weights);
        result /= totalWeight;
#ifdef DEBUG
        fprintf(stderr, "Output %d = %g\n", k, result);
#endif // DEBUG
    }

#endif // INCLUDE_METHOD2

#ifdef INCLUDE_METHOD3

    /* METHOD 3: Compute results directly with one pass through the data */

    to   (RESULT.cell(k) result)
    from (TRAINDATA trainData, TRAINX trainX,
          TRAINY trainY, TRAININDICES trainIndices,
          TESTDATA testData, TESTX testX, TESTINDICES.cell(k) testIndex,
          DIRFLAGS dirFlags, KERNELWIDTHS kernelWidths, MASKWIDTH maskWidth)
    {
        bool skipIndexFlag;
        IndexT i, j, dirFlag, trainIndex;
        ElementT outputVal, kernelWidth, trainPoint, testPoint, diff, sum,
                 weight, partial, totalPartial, totalWeight;

        // loop over training points
        totalPartial = totalWeight = 0;
        for (j = 0; j < trainIndices.count(); ++j) {

            trainIndex = trainIndices.cell(j);
            outputVal = lookup(trainData, trainY, trainIndex);

            // skip current training index if it is too close to the test
            // index or if the training output value is a NAN
            if ((trainIndex > testIndex - maskWidth &&
                 trainIndex < testIndex + maskWidth) ||
                ISNAN(outputVal)) {
                continue;
            }

            // loop over predictor variables to calculate training weight
            sum = 0;
            skipIndexFlag = false;
            for (i = 0; i < trainX.size(1); ++i) {
                dirFlag     =     dirFlags.cell(trainX.cell(0, i));
                kernelWidth = kernelWidths.cell(trainX.cell(0, i));
                trainPoint = lookup(trainData, trainX.row(i), trainIndex);
                testPoint  = lookup( testData,  testX.row(i),  testIndex);

                // skip this variable if nan is detected in the test point
                if (ISNAN(testPoint)) { continue; }

                // skip this training index if nan is detected
                if (ISNAN(trainPoint)) { skipIndexFlag = true; break; }

                // compute diff (and wrap if dirFlag is true)
                diff = trainPoint - testPoint;
                if (dirFlag) { diff = wrapWindDirDiff(diff); }

                // normalize and save squared difference
                diff /= kernelWidth;
                sum += diff * diff;
            }

            if (skipIndexFlag) { continue; }

            weight = exp(-((ElementT) sum));
            partial = weight * outputVal;

#ifdef DEBUG
            fprintf(stderr, "method 3:  weight(%d, %d) = %g\n", j, k, weight);
            fprintf(stderr, "method 3: partial(%d, %d) = %g\n", j, k, partial);
#endif // DEBUG
            totalWeight += weight;
            totalPartial += partial;
        }

        result = totalPartial / totalWeight;
#ifdef DEBUG
        fprintf(stderr, "Output %d = %g\n", k, result);
#endif // DEBUG
    }
#endif // INCLUDE_METHOD3
}

transform NWKDERecursive
from TRAINDATA[m,n], TRAINX[2,p], TRAINY[2], TRAININDICES[l],
     TESTDATA[m2,n2], TESTX[2,p], TESTINDICES[q],
     DIRFLAGS[m], KERNELWIDTHS[m], MASKWIDTH
to RESULT[q]
tunable recursiveCutoff
{
    to   (RESULT result)
    from (TRAINDATA trainData, TRAINX trainX,
          TRAINY trainY, TRAININDICES trainIndices,
          TESTDATA testData, TESTX testX, TESTINDICES testIndices,
          DIRFLAGS dirFlags, KERNELWIDTHS kernelWidths, MASKWIDTH maskWidth)
    {
        if (q <= MAX(1, recursiveCutoff)) {
            NWKDEBase(result, trainData, trainX, trainY, trainIndices,
                      testData, testX, testIndices,
                      dirFlags, kernelWidths, maskWidth);
            return;
        }

        int mid = q / 2;
        spawn NWKDERecursive(result.region(0, mid),
                             trainData, trainX, trainY, trainIndices,
                             testData, testX, testIndices.region(0, mid),
                             dirFlags, kernelWidths, maskWidth);
        spawn NWKDERecursive(result.region(mid, q),
                             trainData, trainX, trainY, trainIndices,
                             testData, testX, testIndices.region(mid, q),
                             dirFlags, kernelWidths, maskWidth);
        sync;
    }
}

transform NWKDE
from TRAINDATA[__M__,__N__], TRAINX[2,__P__], TRAINY[2], TRAININDICES[__L__],
     TESTDATA[__M2__,__N2__], TESTX[2,__P__], TESTINDICES[__Q__],
     DIRFLAGS[__M__], KERNELWIDTHS[__M__], MASKWIDTH
to RESULT[__Q__]
through INPUTSCHECKED
generator NWKDEGenerator2
accuracy_metric NWKDEMetric2
{
    to   (RESULT result)
    from (TRAINDATA trainData, TRAINX trainX,
          TRAINY trainY, TRAININDICES trainIndices,
          TESTDATA testData, TESTX testX, TESTINDICES testIndices,
          DIRFLAGS dirFlags, KERNELWIDTHS kernelWidths, MASKWIDTH maskWidth)
    {
#ifdef DEBUG
        ElementT ret;
        NWKDECheckInputs(ret, trainData, trainX, trainY, trainIndices,
                         testData, testX, testIndices,
                         dirFlags, kernelWidths, maskWidth);
#endif
        NWKDERecursive(result, trainData, trainX, trainY, trainIndices,
                       testData, testX, testIndices,
                       dirFlags, kernelWidths, maskWidth);
    }
}

#endif // NWKDE_PBCC
